{"cells":[{"cell_type":"markdown","metadata":{"id":"6z4CZeVOowB4"},"source":["# MEViT\n","\n","This is the 11th iteration of MEViT.\n","\n","In this iteration I have included the following changes:\n","\n","- scale factor of 3x\n","- 5% of dataset\n","- learning rate of .01\n","\n","\n","\n","**NOTE:** Connect to a High-RAM Environment"]},{"cell_type":"markdown","metadata":{"id":"szU0egT7tBFP"},"source":["# Training"]},{"cell_type":"markdown","metadata":{"id":"5mU7Z6Bm3MWK"},"source":["Settings"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1717811954333,"user":{"displayName":"Zachary Luttrell","userId":"00155576860657313600"},"user_tz":420},"id":"-7ruruAJ3Noc","outputId":"88f42d10-31e1-45d5-88b6-2172cce245e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model Name: mevit_model_12\n","Percentage to Load: 5.0%\n","Threshold: 0.5\n","Scale Factor: 4.0\n","Initial Learning Rate: 0.01\n"]}],"source":["# MODEL SETTINGS\n","model_name = 'mevit_model_12'\n","percentage_to_load = 5 / 100.0  # Set the % of dataset to be loaded here.\n","threshold = 0.5  # Set the threshold for metrics here\n","scale_factor = 4.0  # Factor by which to scale images and masks\n","initial_lr = 0.01  # Learning rate at the beginning of training\n","print(f'Model Name: {model_name}')\n","print(f'Percentage to Load: {percentage_to_load*100}%')\n","print(f'Threshold: {threshold}')\n","print(f'Scale Factor: {scale_factor}')\n","print(f'Initial Learning Rate: {initial_lr}')"]},{"cell_type":"markdown","metadata":{"id":"8nAk5_E-pMPE"},"source":["Imports"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23022,"status":"ok","timestamp":1717811977351,"user":{"displayName":"Zachary Luttrell","userId":"00155576860657313600"},"user_tz":420},"id":"YgBD9gDcovEY","outputId":"e80d85ab-01fc-4dcc-ab92-16d155313e15"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n","Num GPUs Available:  1\n"]}],"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/gdrive')\n","\n","# Imports\n","import os\n","import cv2\n","import numpy as np\n","import tifffile as tiff\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","from tensorflow.keras import layers, models\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.layers import LayerNormalization, Dense, Dropout, Flatten, Conv2DTranspose\n","from tensorflow.keras.models import Model\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","\n","print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"]},{"cell_type":"markdown","metadata":{"id":"0Kt6_4oqpOKr"},"source":["Data Loading"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7SO8PR--pASc","outputId":"0a20e233-8ae7-4495-fc24-157c20f59124","executionInfo":{"status":"ok","timestamp":1717812218608,"user_tz":420,"elapsed":241267,"user":{"displayName":"Zachary Luttrell","userId":"00155576860657313600"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of image files: 4787\n","Number of mask files: 4787\n","Attempting to load 5.0% of dataset: 239 images/mask pairs...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 239/239 [03:17<00:00,  1.21it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Loading complete.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# Function to count files\n","def count_files(directory, extension=\".tif\"):\n","    return sum(1 for file in os.listdir(directory) if file.endswith(extension))\n","\n","# Paths to your image and mask directories\n","image_directory = '/gdrive/My Drive/Dataset/patches/train/images/'\n","mask_directory = '/gdrive/My Drive/Dataset/patches/train/masks/'\n","\n","# Counting the TIFF files in both directories\n","image_count = count_files(image_directory)\n","mask_count = count_files(mask_directory)\n","\n","print(f\"Number of image files: {image_count}\")\n","print(f\"Number of mask files: {mask_count}\")\n","\n","# Load the images and masks into the lists below\n","images = []\n","masks = []\n","\n","# Sort filenames to ensure matching pairs align\n","image_files = sorted([f for f in os.listdir(image_directory) if f.endswith(\".tif\")])\n","mask_files = sorted([f for f in os.listdir(mask_directory) if f.endswith(\".tif\")])\n","\n","# Determine how many files to load based on percentage\n","number_of_files_to_load = int(len(image_files) * percentage_to_load)\n","\n","# Create a mapping of image names to their corresponding mask names by removing '_Buildings'\n","image_to_mask = {f: f.replace(\"_patch\", \"_Buildings_patch\") for f in image_files}\n","\n","# Only iterate over the subset of files determined by the percentage\n","print(f'Attempting to load {percentage_to_load*100}% of dataset: {number_of_files_to_load} images/mask pairs...')\n","for count, image_name in enumerate(tqdm(image_files[:number_of_files_to_load]), start=1):\n","    try:\n","        img_path = os.path.join(image_directory, image_name)\n","        mask_name = image_to_mask[image_name]\n","        mask_path = os.path.join(mask_directory, mask_name)\n","\n","        if os.path.exists(mask_path):\n","            img = tiff.imread(img_path)\n","            mask = tiff.imread(mask_path)\n","\n","            images.append(img)\n","            masks.append(mask)\n","        else:\n","            print(f'\\nMask not found for image: {image_name}')\n","    except Exception as e:\n","        print(f'\\nError loading image/mask pair {image_name}: {e}')\n","\n","print(\"\\nLoading complete.\")\n","\n","# Check if the number of images and masks loaded are equal\n","if len(images) != len(masks):\n","    raise ValueError(\"The number of loaded images and masks do not match!\")"]},{"cell_type":"markdown","metadata":{"id":"SKX6H6GnpaRB"},"source":["Normalization & Train/Test/Val Split"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"XyWjq5-vpcif","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717812223178,"user_tz":420,"elapsed":4583,"user":{"displayName":"Zachary Luttrell","userId":"00155576860657313600"}},"outputId":"5e844819-c879-422d-a970-94113446a25d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Original image shape: (239, 256, 256, 3)\n","New size: (1024, 1024)\n","Resizing images and masks...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 239/239 [00:00<00:00, 354.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Dataset Shapes:\n","X_train: (171, 1024, 1024, 3), y_train: (171, 1024, 1024, 1)\n","X_val:   (20, 1024, 1024, 3), y_val: (20, 1024, 1024, 1)\n","X_test:  (48, 1024, 1024, 3), y_test: (48, 1024, 1024, 1)\n"]}],"source":["# Function to resize and normalize images and masks\n","def preprocess_images_and_masks(images, masks, scale_factor):\n","    # Squeeze unnecessary singleton dimensions\n","    if images.ndim > 3 and images.shape[1] == 1:\n","        images = images.squeeze(axis=1)\n","    if masks.ndim > 3 and masks.shape[1] == 1:\n","        masks = masks.squeeze(axis=1)\n","\n","    print(f\"Original image shape: {images.shape}\")\n","    new_size = (int(images.shape[1] * scale_factor), int(images.shape[2] * scale_factor))\n","    print(f\"New size: {new_size}\")\n","\n","    if new_size[0] <= 0 or new_size[1] <= 0:\n","        raise ValueError(\"Scale factor resulted in invalid size. Check the scale factor and original dimensions.\")\n","\n","    resized_images = []\n","    resized_masks = []\n","    print(\"Resizing images and masks...\")\n","    for img, mask in tqdm(zip(images, masks), total=len(images)):\n","\n","        try:\n","            resized_img = cv2.resize(img, new_size)\n","            resized_mask = cv2.resize(mask, new_size, interpolation=cv2.INTER_NEAREST)\n","            resized_images.append(resized_img)\n","            resized_masks.append(resized_mask)\n","        except Exception as e:\n","            print(f\"Error resizing image/mask pair: {e}\")\n","            raise\n","\n","    resized_images = np.array(resized_images, dtype=np.float32)\n","    resized_masks = np.array(resized_masks, dtype=np.float32)\n","\n","    # Normalize images and masks\n","    resized_images /= 255.0\n","    resized_masks /= 255.0\n","    resized_masks = resized_masks.reshape((-1, new_size[0], new_size[1], 1))\n","\n","    return resized_images, resized_masks\n","\n","# Convert the lists to numpy arrays\n","images = np.array(images, dtype=np.float32)\n","masks = np.array(masks, dtype=np.float32)\n","\n","# Resize and normalize images and masks\n","images, masks = preprocess_images_and_masks(images, masks, scale_factor)\n","\n","# Split the data into training and test sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(images, masks, test_size=0.2, random_state=1995)\n","\n","# Further split the training set into training and validation sets (90% train, 10% val)\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=1995)\n","\n","# Display the shapes of the datasets to confirm correct dimensions\n","print('Dataset Shapes:')\n","print(f'X_train: {X_train.shape}, y_train: {y_train.shape}')\n","print(f'X_val:   {X_val.shape}, y_val: {y_val.shape}')\n","print(f'X_test:  {X_test.shape}, y_test: {y_test.shape}')"]},{"cell_type":"markdown","metadata":{"id":"85MNZvtrpuar"},"source":["Metrics & Loss"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"MRgTuRcepv8d","colab":{"base_uri":"https://localhost:8080/","height":490},"executionInfo":{"status":"ok","timestamp":1717812281774,"user_tz":420,"elapsed":58601,"user":{"displayName":"Zachary Luttrell","userId":"00155576860657313600"}},"outputId":"a280915c-7a45-47eb-cf58-0d76171254ae"},"outputs":[{"output_type":"stream","name":"stdout","text":["Class weights: {0: 0.5274949720047702, 1: 9.592571542048715}\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["from sklearn.utils.class_weight import compute_class_weight\n","from keras.callbacks import LearningRateScheduler\n","import matplotlib.pyplot as plt\n","\n","# Custom Metrics\n","def dice_coefficient(y_true, y_pred, threshold=threshold):\n","    y_true_f = tf.reshape(y_true, [-1])\n","    y_pred_f = tf.cast(tf.reshape(y_pred, [-1]) > threshold, tf.float32)\n","    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n","    return (2. * intersection + tf.keras.backend.epsilon()) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + tf.keras.backend.epsilon())\n","\n","def jaccard_index(y_true, y_pred, threshold=threshold):\n","    y_true_f = tf.reshape(y_true, [-1])\n","    y_pred_f = tf.cast(tf.reshape(y_pred, [-1]) > threshold, tf.float32)\n","    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n","    union = tf.reduce_sum(y_true_f + y_pred_f) - intersection\n","    return (intersection + tf.keras.backend.epsilon()) / (union + tf.keras.backend.epsilon())\n","\n","def sensitivity(y_true, y_pred, threshold=threshold):\n","    y_true_f = tf.reshape(y_true, [-1])\n","    y_pred_f = tf.cast(tf.reshape(y_pred, [-1]) > threshold, tf.float32)\n","    true_positives = tf.reduce_sum(y_true_f * y_pred_f)\n","    possible_positives = tf.reduce_sum(y_true_f)\n","    return true_positives / (possible_positives + tf.keras.backend.epsilon())\n","\n","def specificity(y_true, y_pred, threshold=threshold):\n","    y_true_f = tf.reshape(y_true, [-1])\n","    y_pred_f = tf.cast(tf.reshape(y_pred, [-1]) > threshold, tf.float32)\n","    true_negatives = tf.reduce_sum((1-y_true_f) * (1-y_pred_f))\n","    possible_negatives = tf.reduce_sum(1-y_true_f)\n","    return true_negatives / (possible_negatives + tf.keras.backend.epsilon())\n","\n","def precision(y_true, y_pred, threshold=threshold):\n","    y_true_f = tf.reshape(y_true, [-1])\n","    y_pred_f = tf.cast(tf.reshape(y_pred, [-1]) > threshold, tf.float32)\n","    true_positives = tf.reduce_sum(y_true_f * y_pred_f)\n","    predicted_positives = tf.reduce_sum(y_pred_f)\n","    return true_positives / (predicted_positives + tf.keras.backend.epsilon())\n","\n","# Loss Function\n","def calculate_class_weights(masks):\n","    flat_labels = masks.flatten().astype(int)\n","    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(flat_labels), y=flat_labels)\n","    class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n","    return class_weights_dict\n","\n","def weighted_binary_crossentropy(weights):\n","    def loss(y_true, y_pred):\n","        bce = tf.keras.backend.binary_crossentropy(y_true, y_pred)\n","        weighted_bce = bce * (weights[1] * y_true + weights[0] * (1 - y_true))\n","        return tf.keras.backend.mean(weighted_bce)\n","    return loss\n","\n","# Calculate the class weights for the training masks\n","class_weights = calculate_class_weights(y_train)\n","print(\"Class weights:\", class_weights)\n","\n","# Visualize class weight distribution\n","plt.bar(class_weights.keys(), class_weights.values())\n","plt.title(\"Class Weight Distribution\")\n","plt.xlabel(\"Class\")\n","plt.ylabel(\"Weight\")\n","plt.show()\n","\n","weighted_loss = weighted_binary_crossentropy(class_weights)"]},{"cell_type":"markdown","source":["Model Definition"],"metadata":{"id":"JeXtfUcd5I8o"}},{"cell_type":"code","source":["# Define the MEViT model\n","class TransformerBlock(layers.Layer):\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n","        super(TransformerBlock, self).__init__()\n","        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.ffn = models.Sequential([\n","            layers.Dense(ff_dim, activation='relu'),\n","            layers.Dense(embed_dim),\n","        ])\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = layers.Dropout(rate)\n","        self.dropout2 = layers.Dropout(rate)\n","\n","    def call(self, inputs, training):\n","        attn_output = self.att(inputs, inputs)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output)\n","\n","def create_vit_segmentation_model(input_shape, num_classes, embed_dim, num_heads, ff_dim):\n","    inputs = layers.Input(shape=input_shape)\n","    # Flatten the input\n","    x = layers.Reshape((-1, input_shape[-1]))(inputs)\n","\n","    # Create patches and embed\n","    patch_size = 16  # Adjust as necessary based on image size and memory constraints\n","    num_patches = (input_shape[0] // patch_size) * (input_shape[1] // patch_size)\n","    patches = layers.Conv2D(filters=embed_dim, kernel_size=patch_size, strides=patch_size, padding=\"valid\")(inputs)\n","    patches = layers.Reshape((num_patches, embed_dim))(patches)\n","\n","    # Add Transformer blocks\n","    for _ in range(8):\n","        patches = TransformerBlock(embed_dim, num_heads, ff_dim)(patches)\n","\n","    # Reshape back to image\n","    x = layers.Reshape((input_shape[0] // patch_size, input_shape[1] // patch_size, embed_dim))(patches)\n","    x = Conv2DTranspose(num_classes, kernel_size=patch_size, strides=patch_size, padding=\"valid\")(x)\n","    outputs = layers.Activation('sigmoid')(x)\n","\n","    return Model(inputs, outputs)"],"metadata":{"id":"PxguaxwF5HuU","executionInfo":{"status":"ok","timestamp":1717812281775,"user_tz":420,"elapsed":16,"user":{"displayName":"Zachary Luttrell","userId":"00155576860657313600"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iVA6ln9zp-Ke"},"source":["Model Compilation"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"yOa_BtwAqWNK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717812283309,"user_tz":420,"elapsed":1548,"user":{"displayName":"Zachary Luttrell","userId":"00155576860657313600"}},"outputId":"b53f1be9-93a6-4059-f1d0-618fbb26b4dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"mevit_model_12\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 1024, 1024, 3)]   0         \n","                                                                 \n"," conv2d (Conv2D)             (None, 64, 64, 64)        49216     \n","                                                                 \n"," reshape_1 (Reshape)         (None, 4096, 64)          0         \n","                                                                 \n"," transformer_block (Transfo  (None, 4096, 64)          83200     \n"," rmerBlock)                                                      \n","                                                                 \n"," transformer_block_1 (Trans  (None, 4096, 64)          83200     \n"," formerBlock)                                                    \n","                                                                 \n"," transformer_block_2 (Trans  (None, 4096, 64)          83200     \n"," formerBlock)                                                    \n","                                                                 \n"," transformer_block_3 (Trans  (None, 4096, 64)          83200     \n"," formerBlock)                                                    \n","                                                                 \n"," transformer_block_4 (Trans  (None, 4096, 64)          83200     \n"," formerBlock)                                                    \n","                                                                 \n"," transformer_block_5 (Trans  (None, 4096, 64)          83200     \n"," formerBlock)                                                    \n","                                                                 \n"," transformer_block_6 (Trans  (None, 4096, 64)          83200     \n"," formerBlock)                                                    \n","                                                                 \n"," transformer_block_7 (Trans  (None, 4096, 64)          83200     \n"," formerBlock)                                                    \n","                                                                 \n"," reshape_2 (Reshape)         (None, 64, 64, 64)        0         \n","                                                                 \n"," conv2d_transpose (Conv2DTr  (None, 1024, 1024, 1)     16385     \n"," anspose)                                                        \n","                                                                 \n"," activation (Activation)     (None, 1024, 1024, 1)     0         \n","                                                                 \n","=================================================================\n","Total params: 731201 (2.79 MB)\n","Trainable params: 731201 (2.79 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["from keras.optimizers import SGD\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","input_shape = (int(256 * scale_factor), int(256 * scale_factor), 3)\n","num_classes = 1\n","embed_dim = 64  # Adjust as necessary\n","num_heads = 4   # Adjust as necessary\n","ff_dim = 128    # Adjust as necessary\n","\n","model = create_vit_segmentation_model(input_shape, num_classes, embed_dim, num_heads, ff_dim)\n","model._name = model_name\n","model.summary()\n","\n","# Learning rate schedule\n","def lr_schedule(epoch, lr):\n","    if epoch >= 50 and epoch < 100:\n","        return lr * 0.99\n","    elif epoch >= 100 and epoch < 125:\n","        return lr * 0.9\n","    return lr\n","\n","lr_scheduler = LearningRateScheduler(lr_schedule)\n","\n","# Compile the model\n","sgd = SGD(learning_rate=initial_lr, momentum=0.9)\n","model.compile(\n","    optimizer=sgd,\n","    loss=weighted_loss,\n","    metrics=['accuracy', dice_coefficient, jaccard_index, sensitivity, specificity, precision]\n",")"]},{"cell_type":"markdown","metadata":{"id":"9pd10BmVsJJc"},"source":["Model Training"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"k8N1wpf-sKt8","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"be92c91a-6e3a-43da-ccd4-5c196800b77f","executionInfo":{"status":"error","timestamp":1717812310160,"user_tz":420,"elapsed":26857,"user":{"displayName":"Zachary Luttrell","userId":"00155576860657313600"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/150\n"]},{"output_type":"error","ename":"ResourceExhaustedError","evalue":"Graph execution error:\n\nDetected at node mevit_model_12/transformer_block/multi_head_attention/einsum/Einsum defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 250, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 748, in __init__\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-8-3b98ee7a9ba4>\", line 19, in <cell line: 19>\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 590, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\", line 515, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\", line 672, in _run_internal_graph\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"<ipython-input-6-1ccccd26c986>\", line 16, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/attention/multi_head_attention.py\", line 600, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/attention/multi_head_attention.py\", line 532, in _compute_attention\n\nOOM when allocating tensor with shape[32,4,4096,4096] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node mevit_model_12/transformer_block/multi_head_attention/einsum/Einsum}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_14599]","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-3b98ee7a9ba4>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Experiment with different values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node mevit_model_12/transformer_block/multi_head_attention/einsum/Einsum defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 377, in dispatch_queue\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 250, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 748, in __init__\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-8-3b98ee7a9ba4>\", line 19, in <cell line: 19>\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 590, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\", line 515, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\", line 672, in _run_internal_graph\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"<ipython-input-6-1ccccd26c986>\", line 16, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/attention/multi_head_attention.py\", line 600, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/attention/multi_head_attention.py\", line 532, in _compute_attention\n\nOOM when allocating tensor with shape[32,4,4096,4096] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node mevit_model_12/transformer_block/multi_head_attention/einsum/Einsum}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_14599]"]}],"source":["# Setup callbacks\n","early_stopping = EarlyStopping(\n","    monitor='val_dice_coefficient',\n","    patience=30,  # Reduced patience\n","    verbose=1,\n","    mode='max',\n","    restore_best_weights=True\n",")\n","\n","model_checkpoint = ModelCheckpoint(\n","    f'/gdrive/My Drive/Dataset/Models/best_{model_name}',  # Path where the model will be saved\n","    monitor='val_dice_coefficient',  # Save the model based on the maximum dice_coefficient value\n","    verbose=1,\n","    save_best_only=True,\n","    mode='max'\n",")\n","\n","# Fit the model\n","history = model.fit(\n","    X_train, y_train,\n","    batch_size=32,  # Experiment with different values\n","    epochs=150,\n","    verbose=1,\n","    validation_data=(X_val, y_val),\n","    callbacks=[model_checkpoint, early_stopping, lr_scheduler]\n",")\n","\n","# Save the model\n","model.save(f'/gdrive/My Drive/Dataset/Models/{model_name}.h5')"]},{"cell_type":"markdown","metadata":{"id":"XpKxuoOXtGOe"},"source":["# Evaluation"]},{"cell_type":"markdown","metadata":{"id":"eEY45m0AsTyF"},"source":["Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A3QbrIXpsU9K","executionInfo":{"status":"aborted","timestamp":1717812310162,"user_tz":420,"elapsed":7,"user":{"displayName":"Zachary Luttrell","userId":"00155576860657313600"}}},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, classification_report\n","import seaborn as sns\n","from sklearn.metrics import roc_auc_score\n","\n","from sklearn.metrics import roc_auc_score\n","\n","# Evaluation functions\n","def plot_training_history(history):\n","    plt.figure(figsize=(12, 6))\n","\n","    plt.subplot(1, 2, 1)\n","    plt.plot(history.history['loss'], label='Training Loss')\n","    plt.plot(history.history['val_loss'], label='Validation Loss')\n","    plt.title('Loss Over Epochs')\n","    plt.legend()\n","\n","    plt.subplot(1, 2, 2)\n","    plt.plot(history.history['dice_coefficient'], label='Training Dice Coefficient')\n","    plt.plot(history.history['val_dice_coefficient'], label='Validation Dice Coefficient')\n","    plt.title('Dice Coefficient Over Epochs')\n","    plt.legend()\n","\n","    plt.show()\n","\n","def show_predictions(X, y_true, y_pred, threshold=threshold, num_samples=5):\n","    indices = np.random.choice(range(len(X)), num_samples, replace=False)\n","\n","    for i in indices:\n","        plt.figure(figsize=(12, 6))\n","        plt.subplot(1, 3, 1)\n","        plt.imshow(X[i])\n","        plt.title(f'Original Image [{i}]')\n","        plt.axis('off')\n","\n","        plt.subplot(1, 3, 2)\n","        plt.imshow(y_true[i].squeeze(), cmap='gray')\n","        plt.title(f'True Mask [{i}]')\n","        plt.axis('off')\n","\n","        plt.subplot(1, 3, 3)\n","        plt.imshow(y_pred[i].squeeze() > threshold, cmap='gray')  # Apply a threshold to convert probabilities to binary mask\n","        plt.title(f'Predicted Mask [{i}]')\n","        plt.axis('off')\n","\n","        plt.show()\n","\n","def plot_confusion_matrix(y_true, y_pred, threshold=threshold):\n","    y_true_f = y_true.flatten()\n","    y_pred_f = (y_pred.flatten() > threshold).astype(int)  # Thresholding probabilities\n","\n","    cm = confusion_matrix(y_true_f, y_pred_f)\n","    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Normalize the confusion matrix\n","\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n","    plt.xlabel('Predicted')\n","    plt.ylabel('True')\n","    plt.title('Confusion Matrix')\n","\n","    labels = ['True Negative (Correct: No Buildings)', 'False Positive (Incorrect: There Was No Building)', 'False Negative (Incorrect: There Was A Building Here)', 'True Positive (Correct: There Is A Building Here)']\n","    counts = [\"{0:0.0f}\".format(value) for value in cm.flatten()]\n","    percentages = [\"{0:.2%}\".format(value) for value in cm_normalized.flatten()]\n","\n","    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(labels, counts, percentages)]\n","    labels = np.asarray(labels).reshape(2, 2)\n","\n","    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', cbar=False, center=0)\n","    plt.xlabel('Predicted')\n","    plt.ylabel('True')\n","    plt.title('Confusion Matrix')\n","    plt.show()\n","\n","# Perform the evaluation\n","plot_training_history(history)\n","y_pred = model.predict(X_test)\n","show_predictions(X_test, y_test, y_pred, threshold=threshold)\n","plot_confusion_matrix(y_test, y_pred, threshold=threshold)\n","print(classification_report(y_test.flatten(), (y_pred.flatten() > threshold).astype(int)))\n","print(\"ROC-AUC:\", roc_auc_score(y_test.flatten(), y_pred.flatten()))"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[{"file_id":"11cQTe7Eol_kFXnjSdggg1tPfc_nCMFZO","timestamp":1717810201173},{"file_id":"14p7aFNTaI5ZSsc4Y0w4fcY3l6VKxkkFW","timestamp":1717396481877},{"file_id":"1NpShU9m-Nj2D6tCdSxCff3TpL1Y5aKF6","timestamp":1717380265657},{"file_id":"1f1lIddKXevfi94mJs3PuxBxnuM8uZQwo","timestamp":1717380004583},{"file_id":"15BvoeeprCeA1pk7MPCqg6cTM9JuV7IMU","timestamp":1717229218582},{"file_id":"1xDjMyzCNWDd78z51V5LG7-Fb94tRFS6c","timestamp":1717202269266},{"file_id":"1T0bDS1xnUbpTn_HKo-QW-hpHKzAiC40F","timestamp":1717097791182},{"file_id":"1pYfR623jCKtZ1BkOrjZDqoZh0AaLR1dq","timestamp":1717085451334},{"file_id":"1Gyjx9u2SBZzf4fzcEZ-dytx7j6pCy_Tz","timestamp":1717046772304},{"file_id":"16GmAaRyICYXTTeqUaZFMWUtzqK1Fw_nR","timestamp":1717043481212},{"file_id":"1elMNkk0w4e6W0gWUEr4QE6xtozqkygu9","timestamp":1717011460322}],"gpuType":"L4","authorship_tag":"ABX9TyPJDSpUW33txbMqLHw+Ajdx"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}