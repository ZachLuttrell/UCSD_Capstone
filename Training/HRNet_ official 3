{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"10EFF9QG6NldvXqwPDl5YcVsPwttHrDRU","timestamp":1716000404950},{"file_id":"1YmtFVUTyht1hQbnlKNstSsAaKflI3108","timestamp":1715999216889},{"file_id":"1BvxA7qCHIZ09v8dEHQ0QgZnJ-DKEM3sJ","timestamp":1715994812265},{"file_id":"1XN7FNC6Ne8zlTk6iZDWCkrInKTPKXguF","timestamp":1715734211144},{"file_id":"1fsz3ig6Yls65veB4C995WmD7u9gf6YGg","timestamp":1715726840635},{"file_id":"1Y4virpbOQI3QDDOmLPiXMqrBHP4oYaym","timestamp":1715418058503}],"machine_shape":"hm","authorship_tag":"ABX9TyMWJDOomfjdoWCECgWyjRz7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**NOTE:**\n","Connect to a High-RAM environment\n","\n","- In this iteration, I attempt to match the lxastro approach as closely as possible, however I did not incorporate the pre-trained weights as they did and instead have decided to attempt to train from scratch.\n","\n","CHANGES:\n","- Upsample Images: Upsample images and masks by 3x before training and inference.\n","- Patch Size: Use 512x512 patches for training and inference.\n","- SGD Optimizer: Implement SGD optimizer with a learning rate schedule."],"metadata":{"id":"2Dtncvn0DrB3"}},{"cell_type":"markdown","source":["Import the libraries"],"metadata":{"id":"ePqZyr0bplAb"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/gdrive')\n","\n","import os\n","import cv2\n","import numpy as np\n","import tifffile as tiff\n","from sklearn.model_selection import train_test_split\n","from skimage.transform import resize"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1giikXrR7_Xa","executionInfo":{"status":"ok","timestamp":1716000897766,"user_tz":420,"elapsed":21474,"user":{"displayName":"Zachary Luttrell","userId":"00155576860657313600"}},"outputId":"53013c49-bd8b-4ca4-9dbb-eb3e3606ade3"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}]},{"cell_type":"markdown","source":["Load the dataset"],"metadata":{"id":"WO8y2eTMpnj1"}},{"cell_type":"code","source":["def count_files(directory, extension=\".tif\"):\n","    # List all files in the directory and count those with the specified extension\n","    return sum(1 for file in os.listdir(directory) if file.endswith(extension))\n","\n","# Paths to your image and mask directories\n","image_directory = '/gdrive/My Drive/Dataset/patches/train/images/'\n","mask_directory = '/gdrive/My Drive/Dataset/patches/train/masks/'\n","\n","# Counting the TIFF files in both directories\n","image_count = count_files(image_directory)\n","mask_count = count_files(mask_directory)\n","\n","print(f\"Number of image files: {image_count}\")\n","print(f\"Number of mask files: {mask_count}\")"],"metadata":{"id":"wehwdeclprgm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716000943254,"user_tz":420,"elapsed":45496,"user":{"displayName":"Zachary Luttrell","userId":"00155576860657313600"}},"outputId":"64251084-037f-4c72-861c-1f1937d029c7"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of image files: 4787\n","Number of mask files: 4787\n"]}]},{"cell_type":"code","source":["# Load the images and masks into the lists below\n","images = []\n","masks = []\n","\n","# Sort filenames to ensure matching pairs align\n","image_files = sorted([f for f in os.listdir(image_directory) if f.endswith(\".tif\")])\n","mask_files = sorted([f for f in os.listdir(mask_directory) if f.endswith(\".tif\")])\n","\n","# Determine how many files to load based on percentage\n","percentage_to_load = 20 / 100.0  # Set the % of dataset to be loaded here.\n","number_of_files_to_load = int(len(image_files) * percentage_to_load)\n","\n","# Create a mapping of image names to their corresponding mask names by removing '_Buildings'\n","image_to_mask = {f: f.replace(\"_patch\", \"_Buildings_patch\") for f in image_files}\n","\n","# Only iterate over the subset of files determined by the percentage\n","print(f'Attempting to load {percentage_to_load*100}% of dataset: {number_of_files_to_load} images/mask pairs...')\n","for count, image_name in enumerate(image_files[:number_of_files_to_load], start=1):\n","    img_path = os.path.join(image_directory, image_name)\n","    mask_name = image_to_mask[image_name]\n","    mask_path = os.path.join(mask_directory, mask_name)\n","\n","    if os.path.exists(mask_path):\n","        img = tiff.imread(img_path)\n","        mask = tiff.imread(mask_path)\n","\n","        images.append(img)\n","        masks.append(mask)\n","    else:\n","        print(f'\\nMask not found for image: {image_name}')\n","\n","    # Update the count in the same line\n","    print(f'\\rProcessed: {count}/{number_of_files_to_load}', end='')\n","\n","print(\"\\nLoading complete.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tt5yTLYd8Of2","outputId":"8112b122-eb9d-4fb2-c7ba-2a773406ac03","executionInfo":{"status":"ok","timestamp":1716001321488,"user_tz":420,"elapsed":378245,"user":{"displayName":"Zachary Luttrell","userId":"00155576860657313600"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Attempting to load 20.0% of dataset: 957 images/mask pairs...\n","Processed: 957/957\n","Loading complete.\n"]}]},{"cell_type":"markdown","source":["Upsampling"],"metadata":{"id":"95G2X05YmkjX"}},{"cell_type":"code","source":["# Upsample images and masks by 3x\n","def upsample_image(image, scale_factor=3):\n","    if image.ndim == 2:\n","        return resize(image, (image.shape[0] * scale_factor, image.shape[1] * scale_factor), mode='reflect', anti_aliasing=True)\n","    elif image.ndim == 3:\n","        return resize(image, (image.shape[0] * scale_factor, image.shape[1] * scale_factor, image.shape[2]), mode='reflect', anti_aliasing=True)\n","    else:\n","        raise ValueError(\"Unsupported image dimension: {}\".format(image.ndim))\n","\n","upsampled_images = [upsample_image(img) for img in images]\n","upsampled_masks = [upsample_image(mask, scale_factor=3) for mask in masks]\n","\n","# Convert lists to numpy arrays\n","upsampled_images = np.array(upsampled_images, dtype=np.float32)\n","upsampled_masks = np.array(upsampled_masks, dtype=np.float32)\n","\n","# Normalize images\n","upsampled_images /= 255.0\n","upsampled_masks /= 255.0\n","\n","# Ensure masks have a single channel\n","if upsampled_masks.ndim == 3:\n","    upsampled_masks = np.expand_dims(upsampled_masks, axis=-1)\n","\n","# Extract patches of 512x512\n","def extract_patches(image, patch_size=512):\n","    patches = []\n","    for i in range(0, image.shape[0], patch_size):\n","        for j in range(0, image.shape[1], patch_size):\n","            patch = image[i:i + patch_size, j:j + patch_size]\n","            if patch.shape[0] == patch_size and patch.shape[1] == patch_size:\n","                patches.append(patch)\n","    return patches\n","\n","patch_size = 512\n","image_patches = [patch for img in upsampled_images for patch in extract_patches(img, patch_size)]\n","mask_patches = [patch for mask in upsampled_masks for patch in extract_patches(mask, patch_size)]\n","\n","# Convert lists to numpy arrays\n","image_patches = np.array(image_patches, dtype=np.float32)\n","mask_patches = np.array(mask_patches, dtype=np.float32)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":374},"id":"DqZJ74evmmJV","executionInfo":{"status":"error","timestamp":1716002131383,"user_tz":420,"elapsed":228,"user":{"displayName":"Zachary Luttrell","userId":"00155576860657313600"}},"outputId":"81afd8b4-2853-46df-f541-10a81715dbdf"},"execution_count":7,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Unsupported image dimension: 4","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-44ced8c618bb>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsupported image dimension: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mupsampled_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mupsample_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mupsampled_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mupsample_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-44ced8c618bb>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsupported image dimension: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mupsampled_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mupsample_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mupsampled_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mupsample_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-44ced8c618bb>\u001b[0m in \u001b[0;36mupsample_image\u001b[0;34m(image, scale_factor)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'reflect'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manti_aliasing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unsupported image dimension: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mupsampled_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mupsample_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Unsupported image dimension: 4"]}]},{"cell_type":"markdown","source":["Perform Train/Test/Val Split"],"metadata":{"id":"va7o5BL18H1p"}},{"cell_type":"code","source":["# Split the data into training and test sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(image_patches, mask_patches, test_size=0.2, random_state=1995)\n","\n","# Further split the training set into training and validation sets (90% train, 10% val)\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=1995)\n","\n","print('Dataset Shapes:')\n","print(f'X_train: {X_train.shape}, y_train: {y_train.shape}')\n","print(f'X_val:   {X_val.shape}, y_val: {y_val.shape}')\n","print(f'X_test:  {X_test.shape}, y_test: {y_test.shape}')"],"metadata":{"id":"vQvBOYmd8Hku","executionInfo":{"status":"aborted","timestamp":1716001782558,"user_tz":420,"elapsed":11,"user":{"displayName":"Zachary Luttrell","userId":"00155576860657313600"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define the model"],"metadata":{"id":"5ItYB4fjpr_h"}},{"cell_type":"code","source":["from keras.layers import Input, Conv2D, BatchNormalization, Activation, UpSampling2D, add, concatenate\n","from keras.models import Model\n","import tensorflow as tf\n","from tensorflow.keras.backend import epsilon\n","from tensorflow.keras.losses import BinaryCrossentropy\n","from sklearn.utils.class_weight import compute_class_weight\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.optimizers import Adam\n","\n","# HRNet implementation provided by https://github.com/niecongchong/HRNet-keras-semantic-segmentation/blob/master/model/seg_hrnet.py\n","def conv3x3(x, out_filters, strides=(1, 1)):\n","    x = Conv2D(out_filters, 3, padding='same', strides=strides, use_bias=False, kernel_initializer='he_normal')(x)\n","    return x\n","\n","def basic_Block(input, out_filters, strides=(1, 1), with_conv_shortcut=False):\n","    x = conv3x3(input, out_filters, strides)\n","    x = BatchNormalization(axis=3)(x)\n","    x = Activation('relu')(x)\n","\n","    x = conv3x3(x, out_filters)\n","    x = BatchNormalization(axis=3)(x)\n","\n","    if with_conv_shortcut:\n","        residual = Conv2D(out_filters, 1, strides=strides, use_bias=False, kernel_initializer='he_normal')(input)\n","        residual = BatchNormalization(axis=3)(residual)\n","        x = add([x, residual])\n","    else:\n","        x = add([x, input])\n","\n","    x = Activation('relu')(x)\n","    return x\n","\n","def bottleneck_Block(input, out_filters, strides=(1, 1), with_conv_shortcut=False):\n","    expansion = 4\n","    de_filters = int(out_filters / expansion)\n","\n","    x = Conv2D(de_filters, 1, use_bias=False, kernel_initializer='he_normal')(input)\n","    x = BatchNormalization(axis=3)(x)\n","    x = Activation('relu')(x)\n","\n","    x = Conv2D(de_filters, 3, strides=strides, padding='same', use_bias=False, kernel_initializer='he_normal')(x)\n","    x = BatchNormalization(axis=3)(x)\n","    x = Activation('relu')(x)\n","\n","    x = Conv2D(out_filters, 1, use_bias=False, kernel_initializer='he_normal')(x)\n","    x = BatchNormalization(axis=3)(x)\n","\n","    if with_conv_shortcut:\n","        residual = Conv2D(out_filters, 1, strides=strides, use_bias=False, kernel_initializer='he_normal')(input)\n","        residual = BatchNormalization(axis=3)(residual)\n","        x = add([x, residual])\n","    else:\n","        x = add([x, input])\n","\n","    x = Activation('relu')(x)\n","    return x\n","\n","def stem_net(input):\n","    x = Conv2D(64, 3, strides=(2, 2), padding='same', use_bias=False, kernel_initializer='he_normal')(input)\n","    x = BatchNormalization(axis=3)(x)\n","    x = Activation('relu')(x)\n","\n","    x = bottleneck_Block(x, 256, with_conv_shortcut=True)\n","    x = bottleneck_Block(x, 256, with_conv_shortcut=False)\n","    x = bottleneck_Block(x, 256, with_conv_shortcut=False)\n","    x = bottleneck_Block(x, 256, with_conv_shortcut=False)\n","\n","    return x\n","\n","def transition_layer1(x, out_filters_list=[32, 64]):\n","    x0 = Conv2D(out_filters_list[0], 3, padding='same', use_bias=False, kernel_initializer='he_normal')(x)\n","    x0 = BatchNormalization(axis=3)(x0)\n","    x0 = Activation('relu')(x0)\n","\n","    x1 = Conv2D(out_filters_list[1], 3, strides=(2, 2),\n","                padding='same', use_bias=False, kernel_initializer='he_normal')(x)\n","    x1 = BatchNormalization(axis=3)(x1)\n","    x1 = Activation('relu')(x1)\n","\n","    return [x0, x1]\n","\n","def make_branch1_0(x, out_filters=32):\n","    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n","    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n","    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n","    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n","    return x\n","\n","def make_branch1_1(x, out_filters=64):\n","    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n","    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n","    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n","    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n","    return x\n","\n","def fuse_layer1(x):\n","    x0_0 = x[0]\n","    x0_1 = Conv2D(32, 1, use_bias=False, kernel_initializer='he_normal')(x[1])\n","    x0_1 = BatchNormalization(axis=3)(x0_1)\n","    x0_1 = UpSampling2D(size=(2, 2))(x0_1)\n","    x0 = add([x0_0, x0_1])\n","\n","    x1_0 = Conv2D(64, 3, strides=(2, 2), padding='same', use_bias=False, kernel_initializer='he_normal')(x[0])\n","    x1_0 = BatchNormalization(axis=3)(x1_0)\n","    x1_1 = x[1]\n","    x1 = add([x1_0, x1_1])\n","    return [x0, x1]\n","\n","def transition_layer2(x, out_filters_list=[32, 64, 128]):\n","    x0 = Conv2D(out_filters_list[0], 3, padding='same', use_bias=False, kernel_initializer='he_normal')(x[0])\n","    x0 = BatchNormalization(axis=3)(x0)\n","    x0 = Activation('relu')(x0)\n","\n","    x1 = Conv2D(out_filters_list[1], 3, padding='same', use_bias=False, kernel_initializer='he_normal')(x[1])\n","    x1 = BatchNormalization(axis=3)(x1)\n","    x1 = Activation('relu')(x1)\n","\n","    x2 = Conv2D(out_filters_list[2], 3, strides=(2, 2),\n","                padding='same', use_bias=False, kernel_initializer='he_normal')(x[1])\n","    x2 = BatchNormalization(axis=3)(x2)\n","    x2 = Activation('relu')(x2)\n","\n","    return [x0, x1, x2]\n","\n","def make_branch2_0(x, out_filters=32):\n","    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n","    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n","    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n","    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n","    return x\n","\n","def make_branch2_1(x, out_filters=64):\n","    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n","    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n","    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n","    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n","    return x\n","\n","def make_branch2_2(x, out_filters=128):\n","    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n","    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n","    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n","    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n","    return x\n","\n","def fuse_layer2(x):\n","    x0_0 = x[0]\n","    x0_1 = Conv2D(32, 1, use_bias=False, kernel_initializer='he_normal')(x[1])\n","    x0_1 = BatchNormalization(axis=3)(x0_1)\n","    x0_1 = UpSampling2D(size=(2, 2))(x0_1)\n","    x0_2 = Conv2D(32, 1, use_bias=False, kernel_initializer='he_normal')(x[2])\n","    x0_2 = BatchNormalization(axis=3)(x0_2)\n","    x0_2 = UpSampling2D(size=(4, 4))(x0_2)\n","    x0 = add([x0_0, x0_1, x0_2])\n","\n","    x1_0 = Conv2D(64, 3, strides=(2, 2), padding='same', use_bias=False, kernel_initializer='he_normal')(x[0])\n","    x1_0 = BatchNormalization(axis=3)(x1_0)\n","    x1_1 = x[1]\n","    x1_2 = Conv2D(64, 1, use_bias=False, kernel_initializer='he_normal')(x[2])\n","    x1_2 = BatchNormalization(axis=3)(x1_2)\n","    x1_2 = UpSampling2D(size=(2, 2))(x1_2)\n","    x1 = add([x1_0, x1_1, x1_2])\n","\n","    x2_0 = Conv2D(32, 3, strides=(2, 2), padding='same', use_bias=False, kernel_initializer='he_normal')(x[0])\n","    x2_0 = BatchNormalization(axis=3)(x2_0)\n","    x2_0 = Activation('relu')(x2_0)\n","    x2_0 = Conv2D(128, 3, strides=(2, 2), padding='same', use_bias=False, kernel_initializer='he_normal')(x2_0)\n","    x2_0 = BatchNormalization(axis=3)(x2_0)\n","    x2_1 = Conv2D(128, 3, strides=(2, 2), padding='same', use_bias=False, kernel_initializer='he_normal')(x[1])\n","    x2_1 = BatchNormalization(axis=3)(x2_1)\n","    x2_2 = x[2]\n","    x2 = add([x2_0, x2_1, x2_2])\n","    return [x0, x1, x2]\n","\n","def transition_layer3(x, out_filters_list=[32, 64, 128, 256]):\n","    x0 = Conv2D(out_filters_list[0], 3, padding='same', use_bias=False, kernel_initializer='he_normal')(x[0])\n","    x0 = BatchNormalization(axis=3)(x0)\n","    x0 = Activation('relu')(x0)\n","\n","    x1 = Conv2D(out_filters_list[1], 3, padding='same', use_bias=False, kernel_initializer='he_normal')(x[1])\n","    x1 = BatchNormalization(axis=3)(x1)\n","    x1 = Activation('relu')(x1)\n","\n","    x2 = Conv2D(out_filters_list[2], 3, padding='same', use_bias=False, kernel_initializer='he_normal')(x[2])\n","    x2 = BatchNormalization(axis=3)(x2)\n","    x2 = Activation('relu')(x2)\n","\n","    x3 = Conv2D(out_filters_list[3], 3, strides=(2, 2),\n","                padding='same', use_bias=False, kernel_initializer='he_normal')(x[2])\n","    x3 = BatchNormalization(axis=3)(x3)\n","    x3 = Activation('relu')(x3)\n","\n","    return [x0, x1, x2, x3]\n","\n","def make_branch3_0(x, out_filters=32):\n","    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n","    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n","    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n","    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n","    return x\n","\n","def make_branch3_1(x, out_filters=64):\n","    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n","    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n","    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n","    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n","    return x\n","\n","def make_branch3_2(x, out_filters=128):\n","    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n","    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n","    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n","    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n","    return x\n","\n","def make_branch3_3(x, out_filters=256):\n","    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n","    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n","    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n","    x = basic_Block(x, out_filters, with_conv_shortcut=False)\n","    return x\n","\n","def fuse_layer3(x):\n","    x0_0 = x[0]\n","    x0_1 = Conv2D(32, 1, use_bias=False, kernel_initializer='he_normal')(x[1])\n","    x0_1 = BatchNormalization(axis=3)(x0_1)\n","    x0_1 = UpSampling2D(size=(2, 2))(x0_1)\n","    x0_2 = Conv2D(32, 1, use_bias=False, kernel_initializer='he_normal')(x[2])\n","    x0_2 = BatchNormalization(axis=3)(x0_2)\n","    x0_2 = UpSampling2D(size=(4, 4))(x0_2)\n","    x0_3 = Conv2D(32, 1, use_bias=False, kernel_initializer='he_normal')(x[3])\n","    x0_3 = BatchNormalization(axis=3)(x0_3)\n","    x0_3 = UpSampling2D(size=(8, 8))(x0_3)\n","    x0 = concatenate([x0_0, x0_1, x0_2, x0_3], axis=-1)\n","    return x0\n","\n","def final_layer(x, classes=1):\n","    x = UpSampling2D(size=(2, 2))(x)\n","    x = Conv2D(classes, 1, use_bias=False, kernel_initializer='he_normal')(x)\n","    x = BatchNormalization(axis=3)(x)\n","    x = Activation('sigmoid', name='Classification')(x)\n","    return x\n","\n","def seg_hrnet(batch_size, height, width, channel, classes):\n","    inputs = Input(batch_shape=(batch_size,) + (height, width, channel))\n","\n","    x = stem_net(inputs)\n","\n","    x = transition_layer1(x)\n","    x0 = make_branch1_0(x[0])\n","    x1 = make_branch1_1(x[1])\n","    x = fuse_layer1([x0, x1])\n","\n","    x = transition_layer2(x)\n","    x0 = make_branch2_0(x[0])\n","    x1 = make_branch2_1(x[1])\n","    x2 = make_branch2_2(x[2])\n","    x = fuse_layer2([x0, x1, x2])\n","\n","    x = transition_layer3(x)\n","    x0 = make_branch3_0(x[0])\n","    x1 = make_branch3_1(x[1])\n","    x2 = make_branch3_2(x[2])\n","    x3 = make_branch3_3(x[3])\n","    x = fuse_layer3([x0, x1, x2, x3])\n","\n","    out = final_layer(x, classes=classes)\n","\n","    model = Model(inputs=inputs, outputs=out)\n","\n","    return model\n","\n","# CUSTOM METRICS\n","def dice_coefficient(y_true, y_pred, threshold=0.5):\n","    y_true_f = tf.reshape(y_true, [-1])\n","    y_pred_f = tf.cast(tf.reshape(y_pred, [-1]) > threshold, tf.float32)\n","    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n","    return (2. * intersection + K.epsilon()) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + K.epsilon())\n","\n","def jaccard_index(y_true, y_pred, threshold=0.5):\n","    y_true_f = tf.reshape(y_true, [-1])\n","    y_pred_f = tf.cast(tf.reshape(y_pred, [-1]) > threshold, tf.float32)\n","    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n","    union = tf.reduce_sum(y_true_f + y_pred_f) - intersection\n","    return (intersection + K.epsilon()) / (union + K.epsilon())\n","\n","def sensitivity(y_true, y_pred):\n","    true_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * y_pred, 0, 1)))\n","    possible_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true, 0, 1)))\n","    return true_positives / (possible_positives + tf.keras.backend.epsilon())\n","\n","def specificity(y_true, y_pred):\n","    true_negatives = tf.reduce_sum(tf.round(tf.clip_by_value((1-y_true) * (1-y_pred), 0, 1)))\n","    possible_negatives = tf.reduce_sum(tf.round(tf.clip_by_value(1-y_true, 0, 1)))\n","    return true_negatives / (possible_negatives + tf.keras.backend.epsilon())\n","\n","def precision(y_true, y_pred):\n","    true_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * y_pred, 0, 1)))\n","    predicted_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_pred, 0, 1)))\n","    return true_positives / (predicted_positives + tf.keras.backend.epsilon())\n","\n","# LOSS FUNCTION\n","def calculate_class_weights(masks):\n","    # Flatten the mask array to get a 1D array of labels\n","    flat_labels = masks.flatten().astype(int)\n","\n","    # Calculate the class weights\n","    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(flat_labels), y=flat_labels)\n","\n","    # Create a dictionary to map class labels to weights\n","    class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n","\n","    return class_weights_dict\n","\n","def weighted_binary_crossentropy(weights):\n","    def loss(y_true, y_pred):\n","        # Calculate the binary cross-entropy loss\n","        bce = K.binary_crossentropy(y_true, y_pred)\n","\n","        # Apply the weights\n","        weighted_bce = bce * (weights[1] * y_true + weights[0] * (1 - y_true))\n","\n","        return K.mean(weighted_bce)\n","\n","    return loss"],"metadata":{"id":"lg0MCY7apul6","executionInfo":{"status":"aborted","timestamp":1716001782559,"user_tz":420,"elapsed":12,"user":{"displayName":"Zachary Luttrell","userId":"00155576860657313600"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Compile the model"],"metadata":{"id":"HcbMGbbspu_D"}},{"cell_type":"code","source":["from keras.optimizers import SGD\n","from keras.callbacks import LearningRateScheduler\n","\n","# SGD optimizer\n","sgd = SGD(learning_rate=0.01, momentum=0.9)\n","\n","# Learning rate schedule\n","def lr_schedule(epoch, lr):\n","    if epoch >= 40 and epoch < 60:\n","        return lr * 0.1\n","    elif epoch >= 60:\n","        return lr * 0.1\n","    return lr\n","\n","lr_scheduler = LearningRateScheduler(lr_schedule)\n","\n","# Calculate the class weights for the training masks\n","class_weights = calculate_class_weights(y_train)\n","print(\"Class weights:\", class_weights)\n","weighted_loss = weighted_binary_crossentropy(class_weights)\n","\n","model = seg_hrnet(batch_size=None, height=512, width=512, channel=3, classes=1)\n","model.compile(\n","    optimizer=sgd,\n","    loss=weighted_loss,\n","    metrics=['accuracy', dice_coefficient, jaccard_index, sensitivity, specificity, precision]\n",")\n","\n","model.summary()"],"metadata":{"id":"8DscFV4Hpw2h","executionInfo":{"status":"aborted","timestamp":1716001782560,"user_tz":420,"elapsed":12,"user":{"displayName":"Zachary Luttrell","userId":"00155576860657313600"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Train"],"metadata":{"id":"W8bPZIgQpxQ_"}},{"cell_type":"code","source":["from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","# Setup early stopping to monitor the 'val_loss' and stop training after 10 epochs of no improvements\n","early_stopping = EarlyStopping(\n","    monitor='val_dice_coefficient',\n","    patience=10,\n","    verbose=1,\n","    restore_best_weights=True  # Restores model weights from the epoch with the best value of the monitored quantity\n",")\n","\n","# Setup model checkpointing to save the best model observed during training for later use\n","model_checkpoint = ModelCheckpoint(\n","    '/gdrive/My Drive/Dataset/Models/best_hrnet_official_3',  # Path where the model will be saved\n","    monitor='val_dice_coefficient',  # Save the model based on the maximum dice_coefficient value\n","    verbose=1,\n","    save_best_only=True,\n","    mode='max'\n",")\n","\n","# Fit the model\n","history = model.fit(\n","    X_train, y_train,\n","    batch_size=16,\n","    epochs=70,\n","    verbose=1,\n","    validation_data=(X_val, y_val),\n","    callbacks=[early_stopping, model_checkpoint, lr_scheduler]\n",")\n","\n","# Save the model\n","model.save('/gdrive/My Drive/Dataset/Models/hrnet_official_3.h5')\n","model.save('/gdrive/My Drive/Dataset/Models/hrnet_official_3')"],"metadata":{"id":"rFK_G73xpzLI","executionInfo":{"status":"aborted","timestamp":1716001782562,"user_tz":420,"elapsed":13,"user":{"displayName":"Zachary Luttrell","userId":"00155576860657313600"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Evaluate"],"metadata":{"id":"m0NhLemEpzcF"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix, classification_report\n","\n","def plot_training_history(history):\n","    plt.figure(figsize=(12, 6))\n","\n","    plt.subplot(1, 2, 1)\n","    plt.plot(history.history['loss'], label='Training Loss')\n","    plt.plot(history.history['val_loss'], label='Validation Loss')\n","    plt.title('Loss Over Epochs')\n","    plt.legend()\n","\n","    plt.subplot(1, 2, 2)\n","    plt.plot(history.history['dice_coefficient'], label='Training Dice Coefficient')\n","    plt.plot(history.history['val_dice_coefficient'], label='Validation Dice Coefficient')\n","    plt.title('Dice Coefficient Over Epochs')\n","    plt.legend()\n","\n","    plt.show()\n","\n","def show_predictions(X, y_true, y_pred, num_samples=5):\n","    indices = np.random.choice(range(len(X)), num_samples, replace=False)\n","\n","    for i in indices:\n","        plt.figure(figsize=(12, 6))\n","        plt.subplot(1, 3, 1)\n","        plt.imshow(X[i])\n","        plt.title(f'Original Image [{i}]')\n","        plt.axis('off')\n","\n","        plt.subplot(1, 3, 2)\n","        plt.imshow(y_true[i].squeeze(), cmap='gray')\n","        plt.title(f'True Mask [{i}]')\n","        plt.axis('off')\n","\n","        plt.subplot(1, 3, 3)\n","        plt.imshow(y_pred[i].squeeze() > 0.5, cmap='gray')  # Apply a threshold to convert probabilities to binary mask\n","        plt.title(f'Predicted Mask [{i}]')\n","        plt.axis('off')\n","\n","        plt.show()\n","\n","def plot_confusion_matrix(y_true, y_pred):\n","    # Flattening the masks\n","    y_true_f = y_true.flatten()\n","    y_pred_f = (y_pred.flatten() > 0.5).astype(int)  # Thresholding probabilities\n","\n","    cm = confusion_matrix(y_true_f, y_pred_f)\n","    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Normalize the confusion matrix\n","\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n","    plt.xlabel('Predicted')\n","    plt.ylabel('True')\n","    plt.title('Confusion Matrix')\n","\n","    # Adding labels for each quadrant\n","    labels = ['True Negative', 'False Positive', 'False Negative', 'True Positive']\n","    counts = [\"{0:0.0f}\".format(value) for value in cm.flatten()]\n","    percentages = [\"{0:.2%}\".format(value) for value in cm_normalized.flatten()]\n","\n","    # Position labels in the center of each quadrant\n","    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(labels, counts, percentages)]\n","    labels = np.asarray(labels).reshape(2,2)\n","\n","    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', cbar=False, center=0)\n","    plt.xlabel('Predicted')\n","    plt.ylabel('True')\n","    plt.title('Confusion Matrix with Detailed Annotations')\n","    plt.show()\n","\n","\n","# Perform the evaluation\n","plot_training_history(history)\n","y_pred = model.predict(X_test)\n","show_predictions(X_test, y_test, y_pred)\n","plot_confusion_matrix(y_test, y_pred)\n","print(classification_report(y_test.flatten(), (y_pred.flatten() > 0.5).astype(int)))"],"metadata":{"id":"Yy0CoSYbp2yK","executionInfo":{"status":"aborted","timestamp":1716001782563,"user_tz":420,"elapsed":12,"user":{"displayName":"Zachary Luttrell","userId":"00155576860657313600"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Summary of HRNet Implementation for Semantic Segmentation of SpaceNet 7 Dataset\n","Objective\n","\n","The goal is to implement and train an HRNet model for the semantic segmentation task of extracting building footprints from downsampled 10m resolution Sentinel 2 imagery. The implementation incorporates several considerations from the winning approach of the \"lxastro\" team in the SpaceNet 7 challenge, excluding the use of pre-trained weights.\n","Key Considerations\n","\n","    Semantic Segmentation:\n","        HRNet is chosen for its ability to maintain high-resolution features, crucial for detecting small buildings.\n","\n","    Data Preprocessing:\n","        Upsampling: Images and masks are upsampled by 3x to preserve small buildings.\n","        Patch Size: Data is processed using 512x512 patches for both training and inference.\n","        Dataset Split: The dataset is split into training (80%) and test sets (20%), with a further split of the training set into training (90%) and validation sets (10%).\n","\n","    Model Training:\n","        Architecture: HRNet is implemented with custom building blocks and layers.\n","        Optimizer: Stochastic Gradient Descent (SGD) optimizer with an initial learning rate of 0.01, reduced by a factor of 10 at 40 and 60 epochs.\n","        Batch Size: Set to 16.\n","        Epochs: Trained for 70 epochs.\n","        Loss Function: Custom weighted binary cross-entropy loss.\n","        Metrics: Custom metrics including dice coefficient, jaccard index, sensitivity, specificity, and precision.\n","        Early Stopping and Checkpointing: Monitored validation dice coefficient to save the best model.\n","\n","    Model Inference:\n","        Process images in non-overlapping 512x512 patches.\n","        Fetch the probability of the building class after the softmax layer and reconstruct the original image size from patches.\n","\n","Detailed Notebook Implementation\n","\n","    Data Loading and Preprocessing:\n","        Mounted Google Drive to access the dataset.\n","        Loaded image and mask pairs.\n","        Upsampled images and masks by 3x.\n","        Extracted 512x512 patches from upsampled images and masks.\n","        Split the dataset into training, validation, and test sets.\n","\n","    HRNet Model Implementation:\n","        Implemented HRNet architecture using Keras, including custom basic and bottleneck blocks, transition layers, branches, fusion layers, and final layer.\n","\n","    Custom Metrics and Loss Functions:\n","        Defined custom metrics: dice coefficient, jaccard index, sensitivity, specificity, precision.\n","        Implemented a custom weighted binary cross-entropy loss function based on class weights calculated from the training masks.\n","\n","    Model Compilation and Training:\n","        Compiled the HRNet model using the SGD optimizer and custom loss/metrics.\n","        Implemented a learning rate scheduler to adjust the learning rate at specified epochs.\n","        Trained the model with early stopping and model checkpointing.\n","\n","    Model Evaluation:\n","        Evaluated the model using custom metrics, confusion matrix, and classification report.\n","        Visualized training history and predictions.\n","\n","Conclusion\n","\n","This notebook integrates the key considerations from the \"lxastro\" approach to train an HRNet model for semantic segmentation of building footprints in the SpaceNet 7 dataset. The implementation includes upsampling images, using 512x512 patches, custom metrics, and a specific training schedule, aimed at achieving high accuracy in detecting small buildings. This detailed summary provides an overview and reference for future modifications and improvements."],"metadata":{"id":"hzL17SjToo5G"}}]}